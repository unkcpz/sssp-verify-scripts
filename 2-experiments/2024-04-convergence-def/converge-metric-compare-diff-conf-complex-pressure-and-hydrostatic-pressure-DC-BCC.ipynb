{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook is to investigate how different convergence properties related to each other. The goal is to find the best properties defined that are materials independent and can be used to predict the convergence of a PP. \n",
    "\n",
    "The properties that are investigated are:\n",
    "- For pressure, compare the complex defined SSSP v1 residue volume and the vannila hydrostatic pressure\n",
    "- For EOS metrics, compare nu wrt AE and nu with ref 200Ry. (Check and assure the guess that delta' and nu are correlated)\n",
    "- Compare pressure and EOS metrics (nu ref 200Ry)\n",
    "- Other pair see if those are correlated or not\n",
    "\n",
    "What I think is, if I tuning the criteria of properties, there will be a cross from A > B to B > A. The different between if A, B are correlated or not is whether their will be a state where A, B are highly linearly correlated. \n",
    "\n",
    "The testing data is generated by running full convergence test in the grid of [20:5:200] Ry for all different properties calculation method, then can extract and construct the properties date from the output.\n",
    "The tested PPs are Hg, Ga, N, Cs, Mn from gbrv, dojo, psl-paw-high and jth, in order to cover PPs from different generated code sources and different type of elements.\n",
    "\n",
    "This notebook is to compare that selected properties is or not dependent on the structure of the material.\n",
    "The AiiDA data is stored at group `SI/convergence-properties-compare/<conf>`. conf is BCC/FCC/SC/DC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida import load_profile\n",
    "import typing as t\n",
    "\n",
    "load_profile(\"2023-08-07\")\n",
    "\n",
    "from aiida import orm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_sssp_workflow.workflows.convergence.pressure import helper_get_volume_from_pressure_birch_murnaghan\n",
    "from aiida_sssp_workflow.calculations.calculate_bands_distance import get_bands_distance\n",
    "\n",
    "def extract_data_scan_list1(node):\n",
    "    pp_label = node.inputs.label.value.split(' ')[-1]\n",
    "    \n",
    "    real_scan_list = []\n",
    "    for wf in node.called:\n",
    "        if wf.process_label == 'ConvergencePressureWorkChain':\n",
    "            lst = []\n",
    "            for wf2 in wf.called:\n",
    "\n",
    "                if wf2.process_label == 'helper_pressure_difference':\n",
    "                    lst.append(wf2)\n",
    "                if wf2.process_label == 'convergence_analysis':\n",
    "                    break\n",
    "            \n",
    "            real_scan_list = wf.outputs.output_parameters_wfc_test.get_dict()['ecutwfc']\n",
    "        \n",
    "        else:\n",
    "            # parse_pseudo_info or _CachingConvergenceWorkChain\n",
    "            continue\n",
    "        \n",
    "    expected_scan_list = list(range(20, 201, 5))\n",
    "    # find what is in expected but not in real\n",
    "    missing = list(set(expected_scan_list) - set(real_scan_list))\n",
    "    if missing:\n",
    "        # raise a warning\n",
    "        print(f\"Warning - the following cutoffs are missing from node {node.pk}: {missing}\")\n",
    "        scan_list = real_scan_list\n",
    "    else:\n",
    "        scan_list = expected_scan_list\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i, wf in enumerate(lst):\n",
    "        cutoff = scan_list[i]\n",
    "\n",
    "        i_para = wf.inputs.input_parameters.get_dict()\n",
    "        r_para = wf.inputs.ref_parameters.get_dict()\n",
    "        V0 = wf.inputs.V0.value\n",
    "        B0 = wf.inputs.B0.value\n",
    "        B1 = wf.inputs.B1.value\n",
    "    \n",
    "        # Get the data\n",
    "        # Ref_200_nu: the nu value w.r.t. the 200 Ry reference\n",
    "        # Ref_200_deltap: the delta_p value w.r.t. the 200 Ry reference\n",
    "        res = get_conv_data1(i_para, r_para, V0, B0, B1)\n",
    "        data[cutoff] = res\n",
    "\n",
    "    return pp_label, data, scan_list\n",
    "\n",
    "def get_conv_data1(i_para, r_para, V0, B0, B1) -> float:\n",
    "    \"\"\"Return AE_delta (per atom), AE_nu, REL_nu (the nu compare to the reference not to AE)\"\"\"\n",
    "    res_pressure = i_para[\"hydrostatic_stress\"]\n",
    "    ref_pressure = r_para[\"hydrostatic_stress\"]\n",
    "    absolute_diff = abs(res_pressure - ref_pressure)\n",
    "    relative_diff = helper_get_volume_from_pressure_birch_murnaghan(\n",
    "        absolute_diff, V0, B0, B1,\n",
    "    )\n",
    "\n",
    "    return relative_diff\n",
    "\n",
    "def extract_data_scan_list2(node):\n",
    "    real_scan_list = []\n",
    "    pp_label = node.inputs.label.value.split(' ')[-1]\n",
    "    for wf in node.called:\n",
    "        if wf.process_label == 'ConvergencePressureWorkChain':\n",
    "            lst = []\n",
    "            for wf2 in wf.called:\n",
    "\n",
    "                if wf2.process_label == 'helper_pressure_difference':\n",
    "                    lst.append(wf2)\n",
    "                if wf2.process_label == 'convergence_analysis':\n",
    "                    break\n",
    "            \n",
    "            real_scan_list = wf.outputs.output_parameters_wfc_test.get_dict()['ecutwfc']\n",
    "        \n",
    "        else:\n",
    "            # parse_pseudo_info or _CachingConvergenceWorkChain\n",
    "            continue\n",
    "        \n",
    "    expected_scan_list = list(range(20, 201, 5))\n",
    "    # find what is in expected but not in real\n",
    "    missing = list(set(expected_scan_list) - set(real_scan_list))\n",
    "    if missing:\n",
    "        # raise a warning\n",
    "        print(f\"Warning - the following cutoffs are missing from node {node.pk}: {missing}\")\n",
    "        scan_list = real_scan_list\n",
    "    else:\n",
    "        scan_list = expected_scan_list\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i, wf in enumerate(lst):\n",
    "        cutoff = scan_list[i]\n",
    "\n",
    "        i_para = wf.inputs.input_parameters.get_dict()\n",
    "        r_para = wf.inputs.ref_parameters.get_dict()\n",
    "        V0 = wf.inputs.V0.value\n",
    "        B0 = wf.inputs.B0.value\n",
    "        B1 = wf.inputs.B1.value\n",
    "    \n",
    "        res = get_conv_data2(i_para, r_para)\n",
    "        data[cutoff] = res\n",
    "\n",
    "    return pp_label, data, scan_list\n",
    "\n",
    "def get_conv_data2(i_para, r_para) -> float:\n",
    "    \"\"\"Return AE_delta (per atom), AE_nu, REL_nu (the nu compare to the reference not to AE)\"\"\"\n",
    "    res_pressure = i_para[\"hydrostatic_stress\"]\n",
    "    ref_pressure = r_para[\"hydrostatic_stress\"]\n",
    "    absolute_diff = abs(res_pressure - ref_pressure)\n",
    "    relative_on_simple_stress = absolute_diff / abs(ref_pressure)\n",
    "\n",
    "    return relative_on_simple_stress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aiida_sssp_workflow.workflows.convergence.pressure import ConvergencePressureWorkChain\n",
    "# from aiida_sssp_workflow.workflows.verifications import VerificationWorkChain\n",
    "\n",
    "# qb = orm.QueryBuilder().append(\n",
    "#     orm.Group,\n",
    "#     filters={\"label\": {\"==\": \"SI/convergence-properties-compare/SC\"}},\n",
    "#     tag=\"group\",\n",
    "# ).append(\n",
    "#     VerificationWorkChain,\n",
    "#     with_group=\"group\",\n",
    "#     tag=\"vc\",\n",
    "# ).append(\n",
    "#     ConvergencePressureWorkChain,\n",
    "#     with_incoming=\"vc\",\n",
    "#     tag=\"wc\",\n",
    "# ).append(\n",
    "#     orm.CalcFunctionNode,\n",
    "#     filters={\n",
    "#         \"attributes.function_name\": \"helper_pressure_difference\",\n",
    "#     },\n",
    "#     tag=\"cf\",\n",
    "#     with_incoming=\"wc\",\n",
    "# ).append(\n",
    "#     orm.Data,\n",
    "#     with_outgoing=\"cf\",\n",
    "#     project=[\"*\"],\n",
    "# ).append(\n",
    "#     orm.Dict,\n",
    "#     with_incoming=\"wc\",\n",
    "#     project=[\"attributes.ecutwfc\"],\n",
    "# )\n",
    "\n",
    "# qb.order_by({orm.CalcFunctionNode: {\"ctime\": \"asc\"}}).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 'SI/convergence-properties-compare/BCC'\n",
    "gs_nodes = []\n",
    "gs_nodes.extend(orm.Group.collection.get(label=g).nodes)\n",
    "    \n",
    "SC_all_data1 = {}\n",
    "SC_all_data2 = {}\n",
    "for node in gs_nodes:\n",
    "    # give a node and the tuple of criteria\n",
    "    # return the deducted cutoffs of A and B\n",
    "    try:\n",
    "        pp_label1, data1, scan_list1 = extract_data_scan_list1(node)\n",
    "        pp_label2, data2, scan_list2 = extract_data_scan_list2(node)\n",
    "        SC_all_data1[node.pk] = {\n",
    "            \"pp_label\": pp_label1,\n",
    "            \"data\": data1,\n",
    "            \"scan_list\": scan_list1    \n",
    "        }\n",
    "        SC_all_data2[node.pk] = {\n",
    "            \"pp_label\": pp_label2,\n",
    "            \"data\": data2,\n",
    "            \"scan_list\": scan_list2    \n",
    "        }\n",
    "    except Exception as e:\n",
    "        #print(f\"Error: {e}\")\n",
    "        #continue\n",
    "        print(node.pk)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 'SI/convergence-properties-compare/DC'\n",
    "gs_nodes = []\n",
    "gs_nodes.extend(orm.Group.collection.get(label=g).nodes)\n",
    "    \n",
    "DC_all_data1 = {}\n",
    "DC_all_data2 = {}\n",
    "for node in gs_nodes:\n",
    "    # give a node and the tuple of criteria\n",
    "    # return the deducted cutoffs of A and B\n",
    "    try:\n",
    "        pp_label1, data1, scan_list1 = extract_data_scan_list1(node)\n",
    "        pp_label2, data2, scan_list2 = extract_data_scan_list2(node)\n",
    "        DC_all_data1[node.pk] = {\n",
    "            \"pp_label\": pp_label1,\n",
    "            \"data\": data1,\n",
    "            \"scan_list\": scan_list1    \n",
    "        }\n",
    "        DC_all_data2[node.pk] = {\n",
    "            \"pp_label\": pp_label2,\n",
    "            \"data\": data2,\n",
    "            \"scan_list\": scan_list2    \n",
    "        }\n",
    "    except Exception as e:\n",
    "        #print(f\"Error: {e}\")\n",
    "        #continue\n",
    "        print(node.pk)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cutoff(data, scan_list, criteria):\n",
    "    \"\"\"Extract the cutoff for pA and pB from a verification workchain\n",
    "\n",
    "    Args:\n",
    "        data (dict): the data extracted from the verification workchain\n",
    "        scan_list (list): the list of cutoffs used in the verification workchain\n",
    "        criteria (tuple): first element is the criteria for pA, second element is the criteria for pB\n",
    "\n",
    "    Returns:\n",
    "        tuple: the cutoff for pA and pB.\n",
    "    \"\"\"\n",
    "    # Get the cutoff of pA and pB\n",
    "    cut = 200\n",
    "    for cutoff in reversed(scan_list):\n",
    "        try: \n",
    "            p = data[cutoff]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if p > criteria:\n",
    "            break\n",
    "\n",
    "        cut = cutoff\n",
    "\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def compute_cutoff(data12_tuple, criteria):\n",
    "    cut_A_lst = []\n",
    "    cut_B_lst = []\n",
    "\n",
    "    all_data1 = data12_tuple[0]\n",
    "    all_data2 = data12_tuple[1]\n",
    "\n",
    "    ## data1\n",
    "    lst_label = []\n",
    "    for node_pk in all_data1:\n",
    "        pp_label = all_data1[node_pk]['pp_label']\n",
    "        lst_label.append(pp_label)\n",
    "\n",
    "\n",
    "    idx_sorted, pp_label_sorted = zip(*sorted(enumerate(lst_label), key=itemgetter(1)))\n",
    "    sorted_pk_lst = [list(all_data1.keys())[i] for i in idx_sorted]\n",
    "\n",
    "    for node_pk in sorted_pk_lst:\n",
    "        data = all_data1[node_pk]['data']\n",
    "        scan_list = all_data1[node_pk]['scan_list']\n",
    "        cut_A = extract_cutoff(data, scan_list, criteria)\n",
    "        cut_A_lst.append(cut_A)\n",
    "    \n",
    "    ## data2\n",
    "    lst_label = []\n",
    "    for node_pk in all_data2:\n",
    "        pp_label = all_data2[node_pk]['pp_label']\n",
    "        lst_label.append(pp_label)\n",
    "\n",
    "    idx_sorted, pp_label_sorted = zip(*sorted(enumerate(lst_label), key=itemgetter(1)))\n",
    "    sorted_pk_lst = [list(all_data2.keys())[i] for i in idx_sorted]\n",
    "\n",
    "    for node_pk in sorted_pk_lst:\n",
    "        data = all_data2[node_pk]['data']\n",
    "        scan_list = all_data2[node_pk]['scan_list']\n",
    "        cut_B = extract_cutoff(data, scan_list, criteria)\n",
    "        cut_B_lst.append(cut_B)\n",
    "        \n",
    "    return cut_A_lst, cut_B_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for plotting (complex pressure)\n",
    "SC_cut_A_lst, DC_cut_A_lst = compute_cutoff(data12_tuple=(SC_all_data1, DC_all_data1), criteria=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "trace_corr_scatter = go.Scatter(x=SC_cut_A_lst, y=DC_cut_A_lst, mode='markers', name='cutoff correlation')\n",
    "trace_xy_line = go.Scatter(x=[0, 200], y=[0, 200], name='x=y')\n",
    "g = go.FigureWidget(data=[trace_corr_scatter, trace_xy_line])\n",
    "g.layout.xaxis.title = 'cutoff pA'\n",
    "g.layout.yaxis.title = 'cutoff pB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab32c9d8f394f91a09e07591c0778f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=0.1, description='pA', max=10.0, step=0.01),)), FigureWidget({…"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pA_slider = ipw.FloatSlider(value=0.1, min=0.00, max=10.0, step=0.01, description='pA')\n",
    "\n",
    "def response(change):\n",
    "    SC_cut_A_lst, DC_cut_A_lst = compute_cutoff(data12_tuple=(SC_all_data1, DC_all_data1), criteria=pA_slider.value)\n",
    "    with g.batch_update():\n",
    "        g.data[0].x = SC_cut_A_lst\n",
    "        g.data[0].y = DC_cut_A_lst\n",
    "        \n",
    "pA_slider.observe(response, names=\"value\")\n",
    "\n",
    "slider_widgets = ipw.HBox([pA_slider])\n",
    "app = ipw.VBox([slider_widgets, g])\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Get data for plotting (simple pressure)\n",
    "SC_cut_B_lst, DC_cut_B_lst = compute_cutoff(data12_tuple=(SC_all_data2, DC_all_data2), criteria=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "trace_corr_scatter = go.Scatter(x=SC_cut_B_lst, y=DC_cut_B_lst, mode='markers', name='cutoff correlation')\n",
    "trace_xy_line = go.Scatter(x=[0, 200], y=[0, 200], name='x=y')\n",
    "g = go.FigureWidget(data=[trace_corr_scatter, trace_xy_line])\n",
    "g.layout.xaxis.title = 'cutoff pA'\n",
    "g.layout.yaxis.title = 'cutoff pB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d51cd4973f74350be69cbfc6e724057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=3.08, description='pA', max=10.0, step=0.01),)), FigureWidget(…"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pB_slider = ipw.FloatSlider(value=0.1, min=0.00, max=10.0, step=0.1, description='pB')\n",
    "\n",
    "def response(change):\n",
    "    SC_cut_B_lst, DC_cut_B_lst = compute_cutoff(data12_tuple=(SC_all_data2, DC_all_data2), criteria=pA_slider.value)\n",
    "    with g.batch_update():\n",
    "        g.data[0].x = SC_cut_B_lst\n",
    "        g.data[0].y = DC_cut_B_lst\n",
    "        \n",
    "pB_slider.observe(response, names=\"value\")\n",
    "\n",
    "slider_widgets = ipw.HBox([pA_slider])\n",
    "app = ipw.VBox([slider_widgets, g])\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
