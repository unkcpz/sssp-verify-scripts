{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook is to investigate how different convergence properties related to each other. The goal is to find the best properties defined that are materials independent and can be used to predict the convergence of a PP. \n",
    "\n",
    "The properties that are investigated are:\n",
    "- For pressure, compare the complex defined SSSP v1 residue volume and the vannila hydrostatic pressure\n",
    "- For EOS metrics, compare nu wrt AE and nu with ref 200Ry. (Check and assure the guess that delta' and nu are correlated)\n",
    "- Compare pressure and EOS metrics (nu ref 200Ry)\n",
    "- Other pair see if those are correlated or not\n",
    "\n",
    "What I think is, if I tuning the criteria of properties, there will be a cross from A > B to B > A. The different between if A, B are correlated or not is whether their will be a state where A, B are highly linearly correlated. \n",
    "\n",
    "The testing data is generated by running full convergence test in the grid of [20:5:200] Ry for all different properties calculation method, then can extract and construct the properties date from the output.\n",
    "The tested PPs are Hg, Ga, N, Cs, Mn from gbrv, dojo, psl-paw-high and jth, in order to cover PPs from different generated code sources and different type of elements.\n",
    "\n",
    "The AiiDA data is stored at group `SI/convergence-properties-compare`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida import load_profile\n",
    "import typing as t\n",
    "\n",
    "load_profile(\"2023-08-07\")\n",
    "\n",
    "from aiida import orm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_sssp_workflow.workflows.convergence.pressure import helper_get_volume_from_pressure_birch_murnaghan\n",
    "from aiida_sssp_workflow.calculations.calculate_metric import rel_errors_vec_length, _calcDelta\n",
    "from aiida_sssp_workflow.calculations.calculate_bands_distance import get_bands_distance\n",
    "\n",
    "paper_scan_list = [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 90, 100, 120, 150, 200]\n",
    "\n",
    "def extract_data_scan_list1(node):\n",
    "    real_scan_list = []\n",
    "    for wf in node.called:\n",
    "        if wf.process_label == 'ConvergenceCohesiveEnergyWorkChain':\n",
    "            lst = []\n",
    "            for wf2 in wf.called:\n",
    "\n",
    "                if wf2.process_label == 'helper_cohesive_energy_difference':\n",
    "                    lst.append(wf2)\n",
    "                if wf2.process_label == 'convergence_analysis':\n",
    "                    break\n",
    "            \n",
    "            real_scan_list = wf.outputs.output_parameters_wfc_test.get_dict()['ecutwfc']\n",
    "        \n",
    "        else:\n",
    "            # parse_pseudo_info or _CachingConvergenceWorkChain\n",
    "            continue\n",
    "        \n",
    "    expected_scan_list = list(range(20, 201, 5))\n",
    "    # find what is in expected but not in real\n",
    "    missing = list(set(expected_scan_list) - set(real_scan_list))\n",
    "    if missing:\n",
    "        # raise a warning\n",
    "        print(f\"Warning - the following cutoffs are missing from node {node.pk}: {missing}\")\n",
    "        scan_list = real_scan_list\n",
    "    else:\n",
    "        scan_list = expected_scan_list\n",
    "\n",
    "    data = {}\n",
    "    \n",
    "    for i, wf in enumerate(lst):\n",
    "        cutoff = scan_list[i]\n",
    "\n",
    "        i_para = wf.inputs.input_parameters.get_dict()\n",
    "        r_para = wf.inputs.ref_parameters.get_dict()\n",
    "    \n",
    "        # Get the data\n",
    "        # Ref_200_nu: the nu value w.r.t. the 200 Ry reference\n",
    "        # Ref_200_deltap: the delta_p value w.r.t. the 200 Ry reference\n",
    "        res = get_conv_data1(i_para, r_para)\n",
    "        data[cutoff] = res\n",
    "\n",
    "    return data, scan_list\n",
    "\n",
    "def get_conv_data1(i_para, r_para) -> float:\n",
    "    res_energy = i_para[\"cohesive_energy_per_atom\"]\n",
    "    ref_energy = r_para[\"cohesive_energy_per_atom\"]\n",
    "    absolute_diff = abs(res_energy - ref_energy) * 1000.0 # in meV\n",
    "    relative_diff = abs((res_energy - ref_energy) / ref_energy) * 100\n",
    "\n",
    "    return absolute_diff\n",
    "    \n",
    "def extract_data_scan_list2(node):\n",
    "    real_scan_list = []\n",
    "    for wf in node.called:\n",
    "        if wf.process_label == 'ConvergenceCohesiveEnergyWorkChain':\n",
    "            lst = []\n",
    "            for wf2 in wf.called:\n",
    "                if wf2.process_label == 'helper_cohesive_energy_difference':\n",
    "                    lst.append(wf2)\n",
    "                if wf2.process_label == 'convergence_analysis':\n",
    "                    break\n",
    "            \n",
    "            real_scan_list = wf.outputs.output_parameters_wfc_test.get_dict()['ecutwfc']\n",
    "        \n",
    "        else:\n",
    "            # parse_pseudo_info or _CachingConvergenceWorkChain\n",
    "            continue\n",
    "        \n",
    "    expected_scan_list = list(range(20, 201, 5))\n",
    "    # find what is in expected but not in real\n",
    "    missing = list(set(expected_scan_list) - set(real_scan_list))\n",
    "    if missing:\n",
    "        # raise a warning\n",
    "        print(f\"Warning - the following cutoffs are missing from node {node.pk}: {missing}\")\n",
    "        scan_list = real_scan_list\n",
    "    else:\n",
    "        scan_list = expected_scan_list\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i, wf in enumerate(lst):\n",
    "        cutoff = scan_list[i]\n",
    "\n",
    "        i_para = wf.inputs.input_parameters.get_dict()\n",
    "        r_para = wf.inputs.ref_parameters.get_dict()\n",
    "    \n",
    "        # Get the data\n",
    "        # Ref_200_nu: the nu value w.r.t. the 200 Ry reference\n",
    "        # Ref_200_deltap: the delta_p value w.r.t. the 200 Ry reference\n",
    "        res = get_conv_data2(i_para, r_para)\n",
    "        data[cutoff] = res\n",
    "\n",
    "    return data, scan_list\n",
    "\n",
    "def get_conv_data2(i_para, r_para) -> float:\n",
    "    res_energy = i_para[\"cohesive_energy_per_atom\"]\n",
    "    ref_energy = r_para[\"cohesive_energy_per_atom\"]\n",
    "    absolute_diff = abs(res_energy - ref_energy) * 1000.0 # in meV\n",
    "    relative_diff = abs((res_energy - ref_energy) / ref_energy) * 100\n",
    "\n",
    "    return relative_diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - the following cutoffs are missing from node 8292113: [25]\n",
      "Warning - the following cutoffs are missing from node 8292113: [25]\n"
     ]
    }
   ],
   "source": [
    "g = 'SI/convergence-properties-compare/DC'\n",
    "gs_nodes = []\n",
    "gs_nodes.extend(orm.Group.collection.get(label=g).nodes)\n",
    "    \n",
    "all_data1 = {}\n",
    "all_data2 = {}\n",
    "for node in gs_nodes:\n",
    "    # give a node and the tuple of criteria\n",
    "    # return the deducted cutoffs of A and B\n",
    "    try:\n",
    "        data1, scan_list1 = extract_data_scan_list1(node)\n",
    "        data2, scan_list2 = extract_data_scan_list2(node)\n",
    "        all_data1[node.pk] = {\n",
    "            \"data\": data1,\n",
    "            \"scan_list\": scan_list1    \n",
    "        }\n",
    "        all_data2[node.pk] = {\n",
    "            \"data\": data2,\n",
    "            \"scan_list\": scan_list2    \n",
    "        }\n",
    "    except Exception as e:\n",
    "        #print(f\"Error: {e}\")\n",
    "        #continue\n",
    "        print(node.pk)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cutoff(data, scan_list, criteria):\n",
    "    \"\"\"Extract the cutoff for pA and pB from a verification workchain\n",
    "\n",
    "    Args:\n",
    "        data (dict): the data extracted from the verification workchain\n",
    "        scan_list (list): the list of cutoffs used in the verification workchain\n",
    "        criteria (tuple): first element is the criteria for pA, second element is the criteria for pB\n",
    "\n",
    "    Returns:\n",
    "        tuple: the cutoff for pA and pB.\n",
    "    \"\"\"\n",
    "    # Get the cutoff of pA and pB\n",
    "    cut = 200\n",
    "    for cutoff in reversed(scan_list):\n",
    "        try: \n",
    "            p = data[cutoff]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if p > criteria:\n",
    "            break\n",
    "\n",
    "        cut = cutoff\n",
    "\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cutoff(data12_tuple, criteria):\n",
    "    cut_A_lst = []\n",
    "    cut_B_lst = []\n",
    "\n",
    "    all_data1 = data12_tuple[0]\n",
    "    all_data2 = data12_tuple[1]\n",
    "    criteria1 = criteria[0]\n",
    "    criteria2 = criteria[1]\n",
    "\n",
    "    for node_pk in all_data1:\n",
    "        data = all_data1[node_pk]['data']\n",
    "        scan_list = all_data1[node_pk]['scan_list']\n",
    "        cut_A = extract_cutoff(data, scan_list, criteria1)\n",
    "        cut_A_lst.append(cut_A)\n",
    "    \n",
    "    for node_pk in all_data2:\n",
    "        data = all_data2[node_pk]['data']\n",
    "        scan_list = all_data2[node_pk]['scan_list']\n",
    "        cut_B = extract_cutoff(data, scan_list, criteria2)\n",
    "        cut_B_lst.append(cut_B)\n",
    "        \n",
    "    return cut_A_lst, cut_B_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for plotting\n",
    "cut_A_lst, cut_B_lst = compute_cutoff(data12_tuple=(all_data1, all_data2), criteria=(0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "trace_corr_scatter = go.Scatter(x=cut_A_lst, y=cut_B_lst, mode='markers', name='cutoff correlation')\n",
    "trace_xy_line = go.Scatter(x=[0, 200], y=[0, 200], name='x=y')\n",
    "g = go.FigureWidget(data=[trace_corr_scatter, trace_xy_line])\n",
    "g.layout.xaxis.title = 'cutoff pA'\n",
    "g.layout.yaxis.title = 'cutoff pB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceb569e6bb44e8d9bead41f372606af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=2.0, description='pA', max=10.0), FloatSlider(value=0.1, descrâ€¦"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pA_slider = ipw.FloatSlider(value=2, min=0.00, max=10.0, step=0.1, description='pA')\n",
    "pB_slider = ipw.FloatSlider(value=0.1, min=0.00, max=2, step=0.1, description='pB')\n",
    "\n",
    "def response(change):\n",
    "    cut_A_lst, cut_B_lst = compute_cutoff(data12_tuple=(all_data1, all_data2), criteria=(pA_slider.value, pB_slider.value))\n",
    "    with g.batch_update():\n",
    "        g.data[0].x = cut_A_lst\n",
    "        g.data[0].y = cut_B_lst\n",
    "        \n",
    "pA_slider.observe(response, names=\"value\")\n",
    "pB_slider.observe(response, names=\"value\")\n",
    "\n",
    "slider_widgets = ipw.HBox([pA_slider, pB_slider])\n",
    "app = ipw.VBox([slider_widgets, g])\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc9bbbed58d4332a77f5893b71d3547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'heatmap',\n",
       "              'uid': '3dd4d47c-5880-40be-bae3-a1e842aeec25',\n",
       "              'x': array([ 0.  ,  0.05,  0.1 , ...,  9.9 ,  9.95, 10.  ]),\n",
       "              'y': array([0.  , 0.02, 0.04, ..., 3.96, 3.98, 4.  ]),\n",
       "              'z': array([[ 0.   , 32.   , 40.625, ..., 75.875, 75.875, 75.875],\n",
       "                          [51.375, 25.875, 19.25 , ..., 24.5  , 24.5  , 24.5  ],\n",
       "                          [58.875, 28.875, 22.25 , ..., 17.   , 17.   , 17.   ],\n",
       "                          ...,\n",
       "                          [83.5  , 51.5  , 42.875, ...,  9.125,  9.125,  9.125],\n",
       "                          [83.625, 51.625, 43.   , ...,  9.25 ,  9.25 ,  9.25 ],\n",
       "                          [83.625, 51.625, 43.   , ...,  9.25 ,  9.25 ,  9.25 ]]),\n",
       "              'zmax': 10,\n",
       "              'zmin': 0}],\n",
       "    'layout': {'template': '...', 'xaxis': {'title': {'text': 'cutoffA'}}, 'yaxis': {'title': {'text': 'cutoffB'}}}\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heatmap\n",
    "import numpy as np\n",
    "\n",
    "def compute_correlation(x, y):\n",
    "    cut_A_lst, cut_B_lst = compute_cutoff(data12_tuple=(all_data1, all_data2), criteria=(x, y))\n",
    "    arr_A = np.array(cut_A_lst)\n",
    "    arr_B = np.array(cut_B_lst)\n",
    "    \n",
    "    z = np.sum(np.abs(arr_A - arr_B) / 40.0)\n",
    "    \n",
    "    return z\n",
    "\n",
    "xxs = np.linspace(0.0, 10, 201)\n",
    "yys = np.linspace(0.0, 4, 201)\n",
    "# get z map from xxs and yys\n",
    "zzs = np.zeros((201, 201))\n",
    "for i, x in enumerate(xxs):\n",
    "    for j, y in enumerate(yys):\n",
    "        zzs[j, i] = compute_correlation(x, y)\n",
    "        \n",
    "fig = go.FigureWidget(data=go.Heatmap(z=zzs, x=xxs, y=yys, zmax=10, zmin=0))\n",
    "fig.layout.xaxis.title = 'cutoffA'\n",
    "fig.layout.yaxis.title = 'cutoffB'\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
